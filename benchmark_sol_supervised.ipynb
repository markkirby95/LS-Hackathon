{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "benchmark_sol_supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markkirby95/LS-Hackathon/blob/main/benchmark_sol_supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdQgNlBqs1hL"
      },
      "source": [
        "# Automated metadata matching\n",
        "\n",
        "Life sciences (LS)/ Clinical research institutes (academic research institutes, pharma companies, hospitals, clinics etc.) across the world are producing large volumes of data from patients. This can range from clinical information such as diagnostic/prognostic data, omic data such as genetic/proteomic/epigenetic screens, pathological data such as MRI scans etc. One of the main objectives in LS research (both academic and industrial) is to gain actionable insights from these data sets, that goes beyond the diagnosis/prognosis of a (group of) patient(s) and provides a deeper understanding of the diseases, as well as shine lights on new therapeutic options. It is becoming apparent, that to gain actionable insights from LS data sets, we need data from a large number of patients. This is achievable, if we could merge datasets from various institutes, which is turning out to be hugely challenging task, simply because different institutes use different standards, units, nomenclature etc. to store data. <br><br>\n",
        "\n",
        "For instance, patient's age is a common clinical parameter recorded by almost all organisations. One institute can name the variable that records patients’ age as 'patient age', another can name the same variable as 'age', others can name it as 'age at diagnosis', 'days since birth', 'years since birth' etc. The values can also be in days, months, years etc. Therefore, to combine data from many institutes (and sometimes within same institutes), it's essential to understand that all of the above variables are recording the same thing, i.e. patient's age, also we need to make sure that the units (days, months, years) of measuring age are homogenised at the time of integration. <br><br>\n",
        "\n",
        "To assist in the above process the National Cancer Institute (NCI) created the concept of CDE (common data element). See https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search for more details. A big data dump of about 69000 CDE elements are provided in 'cde_database\\full_database' folder in XML format, if you want to further explore. They provide a standard format of representing Life Science's data. This gives us standard variable name, the permissible values , units etc. for each of these clinical parameters. Some research organizations are following this standard, but vast majority aren't. Additionally, there is huge amount of data produced until now which are not standardized using CDEs. <br><br>\n",
        "\n",
        "To be able to integrate data from various institutes, we need to be able to match the variable names in the clinical datasets to the corresponding CDE elements. Currently, there is a drive for developing ML/AI algorithms to achieve this.<br><br>\n",
        "\n",
        "\n",
        "The code below is an initial attempt in this direction. In summary, it tries to match the variables names (generally the column headers in a clinical data file) and values (the column values) of the clinical parameters in a dataset, to the long variable names, and permissible values of the CDE elements. The objective is to find the CDE elements that closely match the each clinical parameter name (i.e. the column header). To do the the following steps are performed: <br><br>\n",
        "\n",
        "1. Converted selected aspects (e.g. long_name, permissible_values etc.) of all CDE elements into numerical vectors using a word embedding model which itself was trained on these data.\n",
        "\n",
        "2. Coverted the clinical parameter names (headers) and values into numerical vector using the same word embedding model as above.\n",
        "\n",
        "3. The vectors from the clinical data can be matched to CDE vectors in a few different ways: <br>\n",
        "   (a) One way is to use unsupervised learning, fit a Nearest Neighbour model to the CDE vectors, and look for the nearest neighbors of each clinical parameter using this model.\n",
        "   (b) Another way is to use supervised learning: Create feature vectors for all possible pairs of clinical parameters and CDE elements, consider the true pairs as positive class (target =1) and the remaining pairs as negative class (target = 0). Train classifiers to this data and use the classifier to evaluate new clinical parameters.\n",
        "   \n",
        " \n",
        "See more details below.\n",
        "   \n",
        "   \n",
        "   \n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stLHWYWXs1hT"
      },
      "source": [
        "## Install custom benchmark solutions libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFeU420ks8By",
        "outputId": "addb90f7-53d7-4379-d4ab-f4f08d9e055d"
      },
      "source": [
        "from google.colab import drive \r\n",
        "drive.mount('/mntDrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /mntDrive; to attempt to forcibly remount, call drive.mount(\"/mntDrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptt-j_Z6s8ti"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/mntDrive/MyDrive/ls_hackathon_files')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-LBLtIEs1hV",
        "outputId": "4104b9e0-54a7-41be-88aa-08d96f83b1d9"
      },
      "source": [
        "!pip install cde_modelling_tools/."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./cde_modelling_tools\n",
            "Building wheels for collected packages: cde-data-modeller\n",
            "  Building wheel for cde-data-modeller (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cde-data-modeller: filename=cde_data_modeller-0.0.1-cp36-none-any.whl size=15977 sha256=8038d970a8c0846c9c580b222453b169862a5775fdd309ebdb456eccddeac1ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t2sa8a5w/wheels/3f/d1/b3/5cb6a69b1427840bba75481bf74323fa872288acc56a7daf25\n",
            "Successfully built cde-data-modeller\n",
            "Installing collected packages: cde-data-modeller\n",
            "  Found existing installation: cde-data-modeller 0.0.1\n",
            "    Uninstalling cde-data-modeller-0.0.1:\n",
            "      Successfully uninstalled cde-data-modeller-0.0.1\n",
            "Successfully installed cde-data-modeller-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOANebhLs1hX",
        "outputId": "94ef6422-04ae-44d9-af48-4953dff014de"
      },
      "source": [
        "# !pip install -r requirements.txt\n",
        "\n",
        "# !pip install gensim --upgrade\n",
        "# !pip install scipy --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.5.4)\n",
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.5.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llIK7MfZs1ha"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjfo5qN5s1hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8581425-0a2a-4e88-99b8-430a638cd51b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import mlflow\n",
        "from cde_modelling.modelling import CDE_data_modeller as cdm\n",
        "from cde_modelling.parsing import TCGA_data_parser as tdp\n",
        "from cde_modelling.utils import Accuracy_calculations as ac\n",
        "import pickle \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
            "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
            "  _deprecated()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n8Pu0yts1hh"
      },
      "source": [
        "## File paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW7_zRr2s1hj"
      },
      "source": [
        "clinical_data_files_dir = 'tcga_training_data/'\n",
        "\n",
        "clinical_data_test_dir = 'tcga_test_data/'\n",
        "\n",
        "cde_database_file = 'cde_database/combined_small_dataset.json'\n",
        "\n",
        "parameter_file = 'params_supervised.json'\n",
        "\n",
        "model_dir = 'models/'\n",
        "\n",
        "# Particiapnts will not have access to this file\n",
        "test_gold_standard = 'gold_standard/test_gs.json'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vDBsAtcs1hk"
      },
      "source": [
        "## Load model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h3OveHvs1hl"
      },
      "source": [
        "# read model parameters\n",
        "params = {}\n",
        "\n",
        "with open(parameter_file,'r') as file:\n",
        "    params = json.load(file)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT1CDUrAs1hl"
      },
      "source": [
        "## Create a fasttext model for the CDE database and index the individual CDE elements in the database \n",
        "\n",
        "Fasttext is a word embedding algorithm developed by FaceBook. Given a corpus, it creates a model that tries to predict if a pair of words appear in the same context. The model first converts the words to a numeric vector which are used as features for the above prediction. We are interested in the feature generation part, i.e. the part which converts words to numeric vectors. For more information on the FastText model see https://radimrehurek.com/gensim/models/fasttext.html.\n",
        "\n",
        "### FastText model training: \n",
        "\n",
        "To train a FastText model we first extracted the long_name and permissible_values of each CDE elements. These were then parsed and cleaned (lower cased, alphanumeric character only, splitted into bag of words). The preprocessed long names and permissible values of all CDE element was considered as the training corpus for the FastText model. The corpus was then used to train A FastText model. The parameters for the model are in the above json file. The trained model is then used to index the CDE elements (i.e. create numeric vectors representing each CDE). We created two sets of vectors for CDE elements, one for the long_names and the other for permissible values. We alo extracted the data_type information for each CDE elements. Below is an example. Let's assume that the following is a (oversimplified) CDE element .\n",
        "CDE_element: \n",
        "{\n",
        "'public_id': 1234\n",
        "'long_name': 'received radiotherapy'\n",
        "..............\n",
        "'permissible_values': ['yes','no']\n",
        "}.\n",
        "\n",
        "To index the above CDE, we performed the following:\n",
        "\n",
        "1. Vectorized the long_name entry (i.e. 'received radiotherapy') using the FastText model. To do that, we vectorized each word (i.e. 'received' and 'radiotherapy') of the long name entry separately. The vectors were then normalized by their L2 norms and averaged. Say for example, the long_name vector is [0.1, 0.345]. \n",
        "\n",
        "2. Vectorized the 'permissible_values' entry (i.e. 'yes', 'no') using the FastText model. To do that, we vectorized each word (i.e. 'yes' and 'no') in the permissible_values entry separately. The vectors were then normalized by their L2 norms and averaged.Say for example, the permissible vector is [0.981, 0.233]. \n",
        "\n",
        "3. We identified whether the permissible values are string or numbers. Note, that for the benchmark solution, we kept this simple. But for the hackathon, the participants can conder more granular data type for example, string, binary, float, int long etc.\n",
        "\n",
        "\n",
        "Combination of the above is used to numerically represent (index) each CDE. The class CDE_data_modeller, in package cde_modelling_tools does the above. Participants should explore using other entries in the CDE data fields to improve their chances of finding a match.\n",
        "\n",
        "The CDE_data_modeller class not only creates the word embedding models and index (vectorize) the CDE data elements, it can also save and load pretrained models and indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "CJmqZWYAs1hn",
        "outputId": "f1226548-233c-46b2-9dbf-3b3e2593dd0e"
      },
      "source": [
        "cde_data_modellers = cdm.CDE_data_modeller(cde_database_file, params)\n",
        "cde_data_modellers.create_model_and_cde_indexes()\n",
        "cde_data_modellers.save_model_and_indexes(model_dir+'fasttext/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading CDE database... please wait\n",
            "Took 0.016711  minutes to load CDE database..\n",
            "Starting model training ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-980372f9bdf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcde_data_modellers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDE_data_modeller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcde_database_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcde_data_modellers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model_and_cde_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcde_data_modellers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_and_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'fasttext/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cde_modelling/modelling/CDE_data_modeller.py\u001b[0m in \u001b[0;36mcreate_model_and_cde_indexes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Step3: Train word embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_word_embedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# Step 4: Vectorize all CDE elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cde_modelling/modelling/CDE_data_modeller.py\u001b[0m in \u001b[0;36mtrain_word_embedding_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'window'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vector_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha)\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    551\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    486\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    487\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVj9ATgs1ho"
      },
      "source": [
        "## Load a pretrained FastText model and saved indexes for CDE elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHGCGI_Vs1hp",
        "outputId": "e54b60ae-2d88-4b01-9df4-93397bf70f24"
      },
      "source": [
        "cde_data_modellers = cdm.CDE_data_modeller(cde_database_file, params)\n",
        "cde_data_modellers.load_model_and_cde_indexes(model_dir+'fasttext')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading CDE database... please wait\n",
            "Took 0.003889  minutes to load CDE database..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZDmUrSRs1hr"
      },
      "source": [
        "## Load and parse training data\n",
        "\n",
        "The training data are a set of clinical data files which records cinical information of patients, e.g. gender, age, disease_type, disease_sub_type, treatment received etc. It's in table format, where the rows represent patients and the columns represent colinical parameters. In case of the training data, the CDE data element corrsponding to each clinical parameter is provided. This information can be used to train machine learning algorithms to predict CDE elements for new clinical parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ4lyxD3s1hs",
        "outputId": "782fabf6-0b24-4190-f57d-4a520a90da34"
      },
      "source": [
        "tdpr = tdp.TCGA_data_processor(clinical_data_files_dir,True )\n",
        "tcga_data = tdpr.get_parsed_data()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 2/171 [00:00<00:09, 17.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Processing clinical metadata.. please wait..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 171/171 [00:05<00:00, 29.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coPC1NErs1hs"
      },
      "source": [
        "The parser returns three types of information for each clinical parameter.\n",
        "\n",
        "1. The name of the parameter (e.g. age, gender, etc.)\n",
        "2. List of values for each parameters (except id columns, continuous variabales etc.)\n",
        "3. Data type of the values. For instance, data type of 'age' is 'number', data type of gender = 'string'. \n",
        "4. A dictionary containing clinical parameters and it's corresponding CDE element ID\n",
        "\n",
        "See the parsed data below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYB7qhYcs1ht",
        "outputId": "459d3b5d-d289-413e-a3a1-925b46c46045"
      },
      "source": [
        "len(tcga_data.keys())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjE73g99s1ht"
      },
      "source": [
        "# tcga_data['headers'] \n",
        "# tcga_data['values']['pharmaceutical_therapy_drug_name']\n",
        "# tcga_data['value_type'] \n",
        "# tcga_data['gold_standard']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D9bhsp-s1hv"
      },
      "source": [
        "## Create base tables for model training\n",
        "\n",
        "To create base tables I performed the following:\n",
        "\n",
        "1. Indexed (vectorized using the FastText model) the headers (clinical parameter names) and values of each clinical parameters parsed in the previous step.\n",
        "2. for each possible pair of clinical_parameter and CDE elements we calculate the following features <br>\n",
        "    (a) Difference between the embedding vectors of the CDE long_name and the clinical parameter name. <br>\n",
        "    (b) Difference between the embedding vectors of the CDE permissible values and the values associated with the clinical parameters in the training dataset. <br>\n",
        "    (c) A similarity measure (cosine similarity, correlation etc.) betwween the CDE long_name and clinical parameter name vectors <br>\n",
        "    (d) A similarity measure (cosine similarity, correlation etc.) betwween the CDE permissible_vaue and observed clinical parameter value vectors <br>\n",
        "    (e) Similarities between the data type of the permissible and observed values of the CDE and the observed clinical parameters respectively <br>\n",
        "\n",
        "3. Note that, in the base table one data point is represented by a pair (clinical parameter and a CDE ). For example: If there are 800 cinical parameters in the training data and 5000 CDE elements in the CDE dataset, the the base table will have 500*8000 = 4million entries. Each entry will have the above features. The 'target' variable is defined as follows: <br>\n",
        "\n",
        "$\n",
        "target = 1, \\text{if the CDE element is manually matched to the clinical parameter} \\\\\n",
        "target = 0, \\text{otherwise}\n",
        "$\n",
        "\n",
        "In the above example, there are 800 clinical parameters, and if only 1 CDE elements is matched to each clinical parameter, the target variable can be equals to 1 in only 800 out of 4 million cases. Therefore the base table is extremely imbalanced. To counter this we need to undersample (or oversample) the abt. The create_abt function in CDE_data_modeller allows undersampling. The ratio of undersampling (number of cases target = 0 / number of cases target =1 ) can be adjusted using the params dictionary. The defalut value is 5 which means in the undersampled base tables, 16.67 % of cases have target =1 and 83.33% of cases have target = 0.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBgzlC2Ts1h0",
        "outputId": "4e87dad1-7434-4983-e68b-d5570ef6b115"
      },
      "source": [
        "abt = cde_data_modellers.create_abt(tcga_data, params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 638/638 [00:00<00:00, 11460.83it/s]\n",
            "  0%|          | 0/627 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start converting descriptors to vectors\n",
            "Took 0.001150 minutes to vectorize the dataset\n",
            "Start converting descriptors to vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 627/627 [00:00<00:00, 716.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Took 0.014697 minutes to vectorize the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k37GJkMfs1h2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1a82ef32-110a-4b70-e0e6-f1e21c7fc8dc"
      },
      "source": [
        "abt.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_vec_0_x</th>\n",
              "      <th>feature_vec_1_x</th>\n",
              "      <th>feature_vec_2_x</th>\n",
              "      <th>feature_vec_3_x</th>\n",
              "      <th>feature_vec_4_x</th>\n",
              "      <th>feature_vec_5_x</th>\n",
              "      <th>feature_vec_6_x</th>\n",
              "      <th>feature_vec_7_x</th>\n",
              "      <th>feature_vec_8_x</th>\n",
              "      <th>feature_vec_9_x</th>\n",
              "      <th>feature_vec_10_x</th>\n",
              "      <th>feature_vec_11_x</th>\n",
              "      <th>feature_vec_12_x</th>\n",
              "      <th>feature_vec_13_x</th>\n",
              "      <th>feature_vec_14_x</th>\n",
              "      <th>feature_vec_15_x</th>\n",
              "      <th>feature_vec_16_x</th>\n",
              "      <th>feature_vec_17_x</th>\n",
              "      <th>feature_vec_18_x</th>\n",
              "      <th>feature_vec_19_x</th>\n",
              "      <th>feature_vec_20_x</th>\n",
              "      <th>feature_vec_21_x</th>\n",
              "      <th>feature_vec_22_x</th>\n",
              "      <th>feature_vec_23_x</th>\n",
              "      <th>feature_vec_24_x</th>\n",
              "      <th>feature_vec_25_x</th>\n",
              "      <th>feature_vec_26_x</th>\n",
              "      <th>feature_vec_27_x</th>\n",
              "      <th>feature_vec_28_x</th>\n",
              "      <th>feature_vec_29_x</th>\n",
              "      <th>feature_vec_30_x</th>\n",
              "      <th>feature_vec_31_x</th>\n",
              "      <th>feature_vec_32_x</th>\n",
              "      <th>feature_vec_33_x</th>\n",
              "      <th>feature_vec_34_x</th>\n",
              "      <th>feature_vec_35_x</th>\n",
              "      <th>feature_vec_36_x</th>\n",
              "      <th>feature_vec_37_x</th>\n",
              "      <th>feature_vec_38_x</th>\n",
              "      <th>feature_vec_39_x</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_vec_14_y</th>\n",
              "      <th>feature_vec_15_y</th>\n",
              "      <th>feature_vec_16_y</th>\n",
              "      <th>feature_vec_17_y</th>\n",
              "      <th>feature_vec_18_y</th>\n",
              "      <th>feature_vec_19_y</th>\n",
              "      <th>feature_vec_20_y</th>\n",
              "      <th>feature_vec_21_y</th>\n",
              "      <th>feature_vec_22_y</th>\n",
              "      <th>feature_vec_23_y</th>\n",
              "      <th>feature_vec_24_y</th>\n",
              "      <th>feature_vec_25_y</th>\n",
              "      <th>feature_vec_26_y</th>\n",
              "      <th>feature_vec_27_y</th>\n",
              "      <th>feature_vec_28_y</th>\n",
              "      <th>feature_vec_29_y</th>\n",
              "      <th>feature_vec_30_y</th>\n",
              "      <th>feature_vec_31_y</th>\n",
              "      <th>feature_vec_32_y</th>\n",
              "      <th>feature_vec_33_y</th>\n",
              "      <th>feature_vec_34_y</th>\n",
              "      <th>feature_vec_35_y</th>\n",
              "      <th>feature_vec_36_y</th>\n",
              "      <th>feature_vec_37_y</th>\n",
              "      <th>feature_vec_38_y</th>\n",
              "      <th>feature_vec_39_y</th>\n",
              "      <th>feature_vec_40_y</th>\n",
              "      <th>feature_vec_41_y</th>\n",
              "      <th>feature_vec_42_y</th>\n",
              "      <th>feature_vec_43_y</th>\n",
              "      <th>feature_vec_44_y</th>\n",
              "      <th>feature_vec_45_y</th>\n",
              "      <th>feature_vec_46_y</th>\n",
              "      <th>feature_vec_47_y</th>\n",
              "      <th>feature_vec_48_y</th>\n",
              "      <th>feature_vec_49_y</th>\n",
              "      <th>header_metrics</th>\n",
              "      <th>value_metrics</th>\n",
              "      <th>metric</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20019</th>\n",
              "      <td>-0.074879</td>\n",
              "      <td>0.044559</td>\n",
              "      <td>-0.151783</td>\n",
              "      <td>-0.005449</td>\n",
              "      <td>-0.010106</td>\n",
              "      <td>-0.151557</td>\n",
              "      <td>-0.010543</td>\n",
              "      <td>-0.081013</td>\n",
              "      <td>-0.057839</td>\n",
              "      <td>0.023783</td>\n",
              "      <td>0.101051</td>\n",
              "      <td>0.034026</td>\n",
              "      <td>0.095180</td>\n",
              "      <td>-0.009005</td>\n",
              "      <td>-0.021036</td>\n",
              "      <td>-0.033139</td>\n",
              "      <td>0.074434</td>\n",
              "      <td>0.095475</td>\n",
              "      <td>-0.080679</td>\n",
              "      <td>-0.031158</td>\n",
              "      <td>0.138429</td>\n",
              "      <td>0.034211</td>\n",
              "      <td>-0.137717</td>\n",
              "      <td>0.112078</td>\n",
              "      <td>-0.120678</td>\n",
              "      <td>-0.010493</td>\n",
              "      <td>-0.056133</td>\n",
              "      <td>-0.116418</td>\n",
              "      <td>-0.056926</td>\n",
              "      <td>0.092470</td>\n",
              "      <td>0.031292</td>\n",
              "      <td>-0.135366</td>\n",
              "      <td>-0.041523</td>\n",
              "      <td>-0.016279</td>\n",
              "      <td>-0.060030</td>\n",
              "      <td>-0.049708</td>\n",
              "      <td>-0.031908</td>\n",
              "      <td>-0.005781</td>\n",
              "      <td>-0.015716</td>\n",
              "      <td>-0.017162</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060029</td>\n",
              "      <td>-0.038849</td>\n",
              "      <td>-0.049657</td>\n",
              "      <td>-0.030440</td>\n",
              "      <td>0.053685</td>\n",
              "      <td>-0.010519</td>\n",
              "      <td>-0.004943</td>\n",
              "      <td>-0.004650</td>\n",
              "      <td>-0.014141</td>\n",
              "      <td>-0.035882</td>\n",
              "      <td>0.077438</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.072247</td>\n",
              "      <td>-0.055844</td>\n",
              "      <td>0.033274</td>\n",
              "      <td>0.010599</td>\n",
              "      <td>0.062551</td>\n",
              "      <td>-0.074481</td>\n",
              "      <td>0.010505</td>\n",
              "      <td>0.028510</td>\n",
              "      <td>0.017844</td>\n",
              "      <td>-0.063890</td>\n",
              "      <td>-0.042891</td>\n",
              "      <td>-0.006392</td>\n",
              "      <td>-0.019003</td>\n",
              "      <td>0.021050</td>\n",
              "      <td>-0.044800</td>\n",
              "      <td>-0.101919</td>\n",
              "      <td>-0.010150</td>\n",
              "      <td>-0.023774</td>\n",
              "      <td>-0.021306</td>\n",
              "      <td>-0.048681</td>\n",
              "      <td>0.033448</td>\n",
              "      <td>-0.000916</td>\n",
              "      <td>-0.069776</td>\n",
              "      <td>-0.012895</td>\n",
              "      <td>0.750432</td>\n",
              "      <td>0.742144</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25204</th>\n",
              "      <td>0.003034</td>\n",
              "      <td>0.001879</td>\n",
              "      <td>0.029784</td>\n",
              "      <td>0.058840</td>\n",
              "      <td>0.013281</td>\n",
              "      <td>0.039684</td>\n",
              "      <td>0.030796</td>\n",
              "      <td>0.025707</td>\n",
              "      <td>0.029569</td>\n",
              "      <td>-0.026366</td>\n",
              "      <td>-0.010668</td>\n",
              "      <td>-0.005446</td>\n",
              "      <td>-0.018721</td>\n",
              "      <td>0.003022</td>\n",
              "      <td>0.015693</td>\n",
              "      <td>-0.046743</td>\n",
              "      <td>-0.071454</td>\n",
              "      <td>-0.063671</td>\n",
              "      <td>0.051582</td>\n",
              "      <td>0.024476</td>\n",
              "      <td>-0.058444</td>\n",
              "      <td>-0.025032</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>-0.016939</td>\n",
              "      <td>0.026204</td>\n",
              "      <td>0.016746</td>\n",
              "      <td>0.055766</td>\n",
              "      <td>-0.007322</td>\n",
              "      <td>-0.020182</td>\n",
              "      <td>-0.044414</td>\n",
              "      <td>0.034586</td>\n",
              "      <td>-0.002434</td>\n",
              "      <td>0.030403</td>\n",
              "      <td>-0.001074</td>\n",
              "      <td>-0.023553</td>\n",
              "      <td>0.037605</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>-0.006679</td>\n",
              "      <td>-0.003921</td>\n",
              "      <td>0.014557</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026237</td>\n",
              "      <td>-0.038230</td>\n",
              "      <td>-0.075120</td>\n",
              "      <td>-0.029289</td>\n",
              "      <td>0.043563</td>\n",
              "      <td>0.023458</td>\n",
              "      <td>0.033413</td>\n",
              "      <td>0.028868</td>\n",
              "      <td>-0.011267</td>\n",
              "      <td>-0.024606</td>\n",
              "      <td>0.061805</td>\n",
              "      <td>0.008816</td>\n",
              "      <td>0.050508</td>\n",
              "      <td>-0.042464</td>\n",
              "      <td>0.042864</td>\n",
              "      <td>-0.036292</td>\n",
              "      <td>0.070791</td>\n",
              "      <td>-0.092961</td>\n",
              "      <td>0.030654</td>\n",
              "      <td>-0.020931</td>\n",
              "      <td>0.038560</td>\n",
              "      <td>-0.078353</td>\n",
              "      <td>-0.078389</td>\n",
              "      <td>0.002150</td>\n",
              "      <td>-0.056652</td>\n",
              "      <td>-0.005379</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>-0.065107</td>\n",
              "      <td>-0.008420</td>\n",
              "      <td>0.009787</td>\n",
              "      <td>-0.018674</td>\n",
              "      <td>0.017608</td>\n",
              "      <td>0.044671</td>\n",
              "      <td>0.021899</td>\n",
              "      <td>-0.089120</td>\n",
              "      <td>-0.008148</td>\n",
              "      <td>0.898956</td>\n",
              "      <td>0.763689</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30658</th>\n",
              "      <td>-0.047950</td>\n",
              "      <td>0.021755</td>\n",
              "      <td>0.013564</td>\n",
              "      <td>-0.008411</td>\n",
              "      <td>0.107960</td>\n",
              "      <td>-0.055107</td>\n",
              "      <td>-0.035221</td>\n",
              "      <td>-0.002115</td>\n",
              "      <td>-0.013555</td>\n",
              "      <td>0.088978</td>\n",
              "      <td>0.061244</td>\n",
              "      <td>-0.058614</td>\n",
              "      <td>0.127698</td>\n",
              "      <td>-0.112143</td>\n",
              "      <td>0.113474</td>\n",
              "      <td>-0.047841</td>\n",
              "      <td>-0.026875</td>\n",
              "      <td>-0.015321</td>\n",
              "      <td>-0.133792</td>\n",
              "      <td>-0.238137</td>\n",
              "      <td>0.019105</td>\n",
              "      <td>-0.034418</td>\n",
              "      <td>-0.103317</td>\n",
              "      <td>0.098528</td>\n",
              "      <td>0.003804</td>\n",
              "      <td>-0.111936</td>\n",
              "      <td>-0.023623</td>\n",
              "      <td>0.039897</td>\n",
              "      <td>0.075875</td>\n",
              "      <td>0.016079</td>\n",
              "      <td>-0.013993</td>\n",
              "      <td>-0.075534</td>\n",
              "      <td>-0.027692</td>\n",
              "      <td>-0.064011</td>\n",
              "      <td>-0.031082</td>\n",
              "      <td>-0.021705</td>\n",
              "      <td>-0.020004</td>\n",
              "      <td>0.013275</td>\n",
              "      <td>-0.037956</td>\n",
              "      <td>-0.053251</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017112</td>\n",
              "      <td>0.021818</td>\n",
              "      <td>0.034223</td>\n",
              "      <td>0.038146</td>\n",
              "      <td>-0.052758</td>\n",
              "      <td>0.028212</td>\n",
              "      <td>0.012051</td>\n",
              "      <td>-0.042634</td>\n",
              "      <td>-0.007044</td>\n",
              "      <td>0.023827</td>\n",
              "      <td>0.006075</td>\n",
              "      <td>0.013293</td>\n",
              "      <td>-0.018091</td>\n",
              "      <td>-0.036617</td>\n",
              "      <td>0.011178</td>\n",
              "      <td>0.042826</td>\n",
              "      <td>-0.009877</td>\n",
              "      <td>0.018936</td>\n",
              "      <td>0.012923</td>\n",
              "      <td>-0.012120</td>\n",
              "      <td>-0.036963</td>\n",
              "      <td>0.022672</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.056427</td>\n",
              "      <td>-0.014021</td>\n",
              "      <td>-0.038900</td>\n",
              "      <td>0.038039</td>\n",
              "      <td>0.010737</td>\n",
              "      <td>-0.032238</td>\n",
              "      <td>-0.004249</td>\n",
              "      <td>-0.001774</td>\n",
              "      <td>0.009726</td>\n",
              "      <td>0.016919</td>\n",
              "      <td>0.019707</td>\n",
              "      <td>0.059432</td>\n",
              "      <td>-0.017012</td>\n",
              "      <td>0.784019</td>\n",
              "      <td>0.745394</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35381</th>\n",
              "      <td>-0.017392</td>\n",
              "      <td>-0.023105</td>\n",
              "      <td>-0.014109</td>\n",
              "      <td>-0.058086</td>\n",
              "      <td>-0.075214</td>\n",
              "      <td>-0.057065</td>\n",
              "      <td>0.057395</td>\n",
              "      <td>-0.120960</td>\n",
              "      <td>0.008653</td>\n",
              "      <td>-0.081426</td>\n",
              "      <td>0.104183</td>\n",
              "      <td>-0.027685</td>\n",
              "      <td>0.020597</td>\n",
              "      <td>0.022957</td>\n",
              "      <td>0.088440</td>\n",
              "      <td>-0.020638</td>\n",
              "      <td>0.065767</td>\n",
              "      <td>-0.015214</td>\n",
              "      <td>-0.017150</td>\n",
              "      <td>-0.001268</td>\n",
              "      <td>0.017427</td>\n",
              "      <td>0.071906</td>\n",
              "      <td>-0.016639</td>\n",
              "      <td>0.069773</td>\n",
              "      <td>-0.025677</td>\n",
              "      <td>0.023823</td>\n",
              "      <td>-0.055449</td>\n",
              "      <td>0.015445</td>\n",
              "      <td>-0.077791</td>\n",
              "      <td>0.013303</td>\n",
              "      <td>0.093452</td>\n",
              "      <td>-0.088513</td>\n",
              "      <td>-0.099616</td>\n",
              "      <td>0.020285</td>\n",
              "      <td>-0.029240</td>\n",
              "      <td>-0.079825</td>\n",
              "      <td>0.061857</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>0.014843</td>\n",
              "      <td>0.081533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083978</td>\n",
              "      <td>-0.092512</td>\n",
              "      <td>0.147799</td>\n",
              "      <td>-0.061280</td>\n",
              "      <td>-0.160395</td>\n",
              "      <td>-0.039630</td>\n",
              "      <td>-0.029381</td>\n",
              "      <td>0.029830</td>\n",
              "      <td>-0.018759</td>\n",
              "      <td>-0.056411</td>\n",
              "      <td>0.036600</td>\n",
              "      <td>0.010373</td>\n",
              "      <td>0.134125</td>\n",
              "      <td>0.015948</td>\n",
              "      <td>-0.035872</td>\n",
              "      <td>0.021081</td>\n",
              "      <td>0.076455</td>\n",
              "      <td>-0.062331</td>\n",
              "      <td>0.007596</td>\n",
              "      <td>0.056801</td>\n",
              "      <td>0.064910</td>\n",
              "      <td>-0.048215</td>\n",
              "      <td>0.042324</td>\n",
              "      <td>-0.031652</td>\n",
              "      <td>-0.048458</td>\n",
              "      <td>-0.063617</td>\n",
              "      <td>-0.121054</td>\n",
              "      <td>0.044985</td>\n",
              "      <td>-0.062831</td>\n",
              "      <td>-0.015858</td>\n",
              "      <td>-0.032520</td>\n",
              "      <td>-0.052521</td>\n",
              "      <td>0.093606</td>\n",
              "      <td>-0.006924</td>\n",
              "      <td>0.041653</td>\n",
              "      <td>-0.000919</td>\n",
              "      <td>0.793131</td>\n",
              "      <td>0.723682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35382</th>\n",
              "      <td>-0.017392</td>\n",
              "      <td>-0.023105</td>\n",
              "      <td>-0.014109</td>\n",
              "      <td>-0.058086</td>\n",
              "      <td>-0.075214</td>\n",
              "      <td>-0.057065</td>\n",
              "      <td>0.057395</td>\n",
              "      <td>-0.120960</td>\n",
              "      <td>0.008653</td>\n",
              "      <td>-0.081426</td>\n",
              "      <td>0.104183</td>\n",
              "      <td>-0.027685</td>\n",
              "      <td>0.020597</td>\n",
              "      <td>0.022957</td>\n",
              "      <td>0.088440</td>\n",
              "      <td>-0.020638</td>\n",
              "      <td>0.065767</td>\n",
              "      <td>-0.015214</td>\n",
              "      <td>-0.017150</td>\n",
              "      <td>-0.001268</td>\n",
              "      <td>0.017427</td>\n",
              "      <td>0.071906</td>\n",
              "      <td>-0.016639</td>\n",
              "      <td>0.069773</td>\n",
              "      <td>-0.025677</td>\n",
              "      <td>0.023823</td>\n",
              "      <td>-0.055449</td>\n",
              "      <td>0.015445</td>\n",
              "      <td>-0.077791</td>\n",
              "      <td>0.013303</td>\n",
              "      <td>0.093452</td>\n",
              "      <td>-0.088513</td>\n",
              "      <td>-0.099616</td>\n",
              "      <td>0.020285</td>\n",
              "      <td>-0.029240</td>\n",
              "      <td>-0.079825</td>\n",
              "      <td>0.061857</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>0.014843</td>\n",
              "      <td>0.081533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083978</td>\n",
              "      <td>-0.092512</td>\n",
              "      <td>0.147799</td>\n",
              "      <td>-0.061280</td>\n",
              "      <td>-0.160395</td>\n",
              "      <td>-0.039630</td>\n",
              "      <td>-0.029381</td>\n",
              "      <td>0.029830</td>\n",
              "      <td>-0.018759</td>\n",
              "      <td>-0.056411</td>\n",
              "      <td>0.036600</td>\n",
              "      <td>0.010373</td>\n",
              "      <td>0.134125</td>\n",
              "      <td>0.015948</td>\n",
              "      <td>-0.035872</td>\n",
              "      <td>0.021081</td>\n",
              "      <td>0.076455</td>\n",
              "      <td>-0.062331</td>\n",
              "      <td>0.007596</td>\n",
              "      <td>0.056801</td>\n",
              "      <td>0.064910</td>\n",
              "      <td>-0.048215</td>\n",
              "      <td>0.042324</td>\n",
              "      <td>-0.031652</td>\n",
              "      <td>-0.048458</td>\n",
              "      <td>-0.063617</td>\n",
              "      <td>-0.121054</td>\n",
              "      <td>0.044985</td>\n",
              "      <td>-0.062831</td>\n",
              "      <td>-0.015858</td>\n",
              "      <td>-0.032520</td>\n",
              "      <td>-0.052521</td>\n",
              "      <td>0.093606</td>\n",
              "      <td>-0.006924</td>\n",
              "      <td>0.041653</td>\n",
              "      <td>-0.000919</td>\n",
              "      <td>0.793131</td>\n",
              "      <td>0.723682</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       feature_vec_0_x  feature_vec_1_x  ...  metric  target\n",
              "20019        -0.074879         0.044559  ...     0.0     1.0\n",
              "25204         0.003034         0.001879  ...     0.0     1.0\n",
              "30658        -0.047950         0.021755  ...     0.0     1.0\n",
              "35381        -0.017392        -0.023105  ...     0.0     1.0\n",
              "35382        -0.017392        -0.023105  ...     1.0     1.0\n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohbz0rf6s1h3"
      },
      "source": [
        "## Train a machine learning model using the abt created above\n",
        "I created a separate class called create_model where a number of supervised and unsupervised learning algorithms are implemented (from sklearn library). The type and parameters of the model can be passed using the params dictionary. \n",
        "\n",
        "!!! Warning: Currently the the two datatype columns in the abt (data_type_string, data_type_number) are complementary and hence redundant. Only one should be used for modelling. This needs to be corrected in the future versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqngVtJhs1h3"
      },
      "source": [
        "# First create feature vectors\n",
        "features = [c for c in abt.columns if ('feature' in c) or ('metric' in c)]\n",
        "\n",
        "# create design matrix and target data\n",
        "X = abt[features]\n",
        "y= abt['target']\n",
        "\n",
        "# create training and validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "\n",
        "\n",
        "# import the Model class\n",
        "from cde_modelling.modelling.create_models import Model\n",
        "\n",
        "# create model\n",
        "model = Model(params)\n",
        "\n",
        "#fit the model\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# calculate the accuracy of the model on the validation data\n",
        "accuracy = model.accuracy(X_val, y_val)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUZ1VR4us1h3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a081ec7-d5a8-44cd-df6e-28b337781d31"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9432717678100264,\n",
              " 'f1': 0.8436363636363636,\n",
              " 'precision': 0.9133858267716536,\n",
              " 'recall': 0.7837837837837838,\n",
              " 'auroc': 0.9715219317678334,\n",
              " 'confusion_matrix': array([[599,  11],\n",
              "        [ 32, 116]]),\n",
              " 'matthews': 0.8127683922146443}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jkiE1ODs1h4"
      },
      "source": [
        "## Test the model on the test dataset\n",
        "\n",
        "To do that, we shall first create the base table for the test dataset by parsing and indexing the test set in the same way as was done for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR9vgesgs1h4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773021dc-f4b5-43f1-911a-0cb3c369d296"
      },
      "source": [
        "tdp1 = tdp.TCGA_data_processor(clinical_data_test_dir,False )\n",
        "test_data = tdp1.get_parsed_data()\n",
        "test_abt =cde_data_modellers.create_abt(test_data)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Processing clinical metadata.. please wait..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  2.86it/s]\n",
            "100%|██████████| 193/193 [00:00<00:00, 7591.46it/s]\n",
            "100%|██████████| 193/193 [00:00<00:00, 1309.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start converting descriptors to vectors\n",
            "Took 0.000511 minutes to vectorize the dataset\n",
            "Start converting descriptors to vectors\n",
            "Took 0.002536 minutes to vectorize the dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeyOEYOhs1h5"
      },
      "source": [
        "## Make predictions for the test dataset using the trained model\n",
        "\n",
        "Note that I have created a model.predict_and_convert_to_json function which returns the prediction in the following format: <br>\n",
        "{\n",
        "clinical parameter1: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
        "clinical parameter2: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
        ".....\n",
        "clinical parametern: [most likely predictions, 2nd most likely prediction, .... , 20th most likely prediction] <br>\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyO7jl_zs1h5"
      },
      "source": [
        "test_abt.fillna(0, inplace = True)\n",
        "\n",
        "index_cols = ['headers','public_id']\n",
        "header_col = index_cols[0]\n",
        "id_col = index_cols [1]\n",
        "\n",
        "results = model.predict_and_convert_to_json(test_abt,20, index_cols, header_col, id_col)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4XyPx2Bs1h5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8bbfefb-5e83-4aa9-dd59-a845071ed8e8"
      },
      "source": [
        "results['abnormal_lymphocyte_percent']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3144381',\n",
              " '2669789',\n",
              " '3151234',\n",
              " '5010431',\n",
              " '2429786',\n",
              " '3233649',\n",
              " '2841253',\n",
              " '4851631',\n",
              " '5028033',\n",
              " '3206020',\n",
              " '3131891',\n",
              " '64773',\n",
              " '2180675',\n",
              " '58301',\n",
              " '2774742',\n",
              " '3107129',\n",
              " '6692791',\n",
              " '2799755',\n",
              " '2497209',\n",
              " '2669788']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPJKbh0-s1h8"
      },
      "source": [
        "# Calculate accuracy of prediction for the test dataset\n",
        "\n",
        "Note that the participants won't have access to the gold standard data, therefore won't be able to perform the following step. However, participants can divide the training data in to train, test, validation sets and perform the following on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2yY4VNBs1h9",
        "outputId": "b43959fb-a635-4282-e7f1-14e0509738d8"
      },
      "source": [
        "test_gs = {}\n",
        "with open(test_gold_standard, 'rb') as file:\n",
        "    test_gs = json.load(file)\n",
        "test_accuracy = ac.calculate_accuracy(test_gs,results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'gold_standard/test_gs.json'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-28-04e33ab9d3ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_gold_standard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_gs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gold_standard/test_gs.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBWe4P_zs1h9",
        "outputId": "a2c869f4-31ff-4c9b-fa4b-87030144fa2c"
      },
      "source": [
        "test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46000000000000013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rzhay0hs1h-"
      },
      "source": [
        "## Log model parameters etc. using mlflow\n",
        "\n",
        "This will ensure reproducibility of results and will keep track of all models and results during the model development and calibration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHhm6Y4vs1h-",
        "outputId": "249cdf12-2a9c-40ab-d0e5-d592b4c01c9a"
      },
      "source": [
        "mlflow.create_experiment('MK_EX')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUlCwIQDs1h_",
        "outputId": "3b17a9f9-3091-4ad5-cc9c-c1b3323e439a"
      },
      "source": [
        "with mlflow.start_run():\n",
        "    # print out current run_uuid\n",
        "    run_uuid = mlflow.active_run().info.run_uuid\n",
        "    print(\"MLflow Run ID: %s\" % run_uuid)\n",
        "    \n",
        "    # log parameters\n",
        "    mlflow.log_param(\"window_size\", params[\"fasttext\"][\"window\"])\n",
        "    mlflow.log_param(\"min_count\", params[\"fasttext\"][\"min_count\"])\n",
        "    mlflow.log_param(\"epochs\", params[\"fasttext\"][\"epochs\"])\n",
        "    mlflow.log_param(\"vector_size\", params[\"fasttext\"][\"vector_size\"])\n",
        "    \n",
        "    \n",
        "    mlflow.log_param(\"features_diference_types\", params[\"features\"][\"differences\"][\"type\"])\n",
        "    mlflow.log_param(\"features_metrics\", params[\"features\"][\"metrics\"][\"metric\"])\n",
        "    mlflow.log_param(\"features_metrics_sim_type\", params[\"features\"][\"metrics\"][\"sim_type\"])\n",
        "    mlflow.log_param(\"features_metrics_scaling\", params[\"features\"][\"metrics\"][\"scaling\"])\n",
        "    mlflow.log_param(\"features_sampling_ratio\", params[\"features\"][\"sampling_ratio\"])\n",
        "    \n",
        "    mlflow.log_param(\"features_samplinf_ratio\", params[\"features\"][\"sampling_ratio\"])\n",
        "    \n",
        "    mlflow.log_param(\"model_type\", params['model'][\"name\"])\n",
        "    \n",
        "    for k in params['model']['model_params'].keys():\n",
        "        mlflow.log_param(\"model_params_\"+k, params['model'][\"model_params\"][k])\n",
        "    \n",
        "    # log metrics\n",
        "        \n",
        "#     mlflow.log_metric(\"test_accuracy\",test_accuracy)\n",
        "#     for k in accuracy.keys():\n",
        "#         if 'confusion' not in k:\n",
        "#             mlflow.log_metric(\"val_accuracy_\"+k,accuracy[k])\n",
        "    \n",
        "    #mlflow.sklearn.logmodel()\n",
        "    with open('models/'+run_uuid+'.pkl','wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "    # Logging the model in MLFlow    \n",
        "    mlflow.log_artifact('models/'+run_uuid+'.pkl')\n",
        "    \n",
        "    mlflow.end_run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLflow Run ID: 3e655116d0c047b2afc3145ca60a9149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ78RKpts1iA"
      },
      "source": [
        "## Use 'mlflow ui' to compare and analyze various model performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBIv3gLYs1iB"
      },
      "source": [
        "<img src=\"img/mlflow_ui.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYcSunyls1iB"
      },
      "source": [
        "!mlflow ui"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMkYskj0s1iC"
      },
      "source": [
        "###  http://localhost:5000"
      ]
    }
  ]
}